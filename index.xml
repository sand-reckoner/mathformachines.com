<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Math for Machines</title>
    <link>https://mathformachines.com/</link>
    <description>Recent content on Math for Machines</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy;Ryan Holbrook 2019</copyright>
    <lastBuildDate>Tue, 14 May 2019 15:19:00 -0500</lastBuildDate>
    
	<atom:link href="https://mathformachines.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A Somewhat Better Retirement Formula (with Calculator!)</title>
      <link>https://mathformachines.com/posts/a-somewhat-better-retirement-formula/</link>
      <pubDate>Tue, 14 May 2019 15:19:00 -0500</pubDate>
      
      <guid>https://mathformachines.com/posts/a-somewhat-better-retirement-formula/</guid>
      <description>&lt;p&gt;&lt;em&gt;This is based off a lesson I put together for a class I teach about general mathematics. I wanted a retirement savings formula that was simple enough for an ordinary person to use on their own, but also flexible enough to account for varied goals or circumstances.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;My formula &lt;a href=&#34;https://www.investopedia.com/articles/personal-finance/092414/retirement-what-percentage-salary-save.asp&#34;&gt;agrees pretty well&lt;/a&gt; with what appear to be experts recommend, and it seems to be fairly robust in simulation. &lt;a href=&#34;https://www.portfoliovisualizer.com/financial-goals?s=y&amp;amp;stages=2&amp;amp;careerYears=41&amp;amp;mode=1&amp;amp;initialAmount=12000&amp;amp;years=70&amp;amp;simulationModel=1&amp;amp;historicalVolatility=true&amp;amp;fullHistory=true&amp;amp;startYear=1972&amp;amp;endYear=2018&amp;amp;bootstrapModel=1&amp;amp;bootstrapMinYears=1&amp;amp;bootstrapMaxYears=20&amp;amp;circularBootstrap=true&amp;amp;distribution=1&amp;amp;dof=30&amp;amp;meanReturn=7.0&amp;amp;volatility=12.0&amp;amp;sequenceStressTest=0&amp;amp;stressTestRetirement=true&amp;amp;inflationModel=1&amp;amp;inflationMean=4.26&amp;amp;inflationVolatility=3.13&amp;amp;customIntervals=false&amp;amp;percentileList=10%2C+25%2C+50%2C+75%2C+90&amp;amp;returnList=0%2C+2.5%2C+5%2C+7.5%2C+10%2C+12.5&amp;amp;asset1=TotalStockMarket&amp;amp;allocation1%5F1=90&amp;amp;asset2=TotalBond&amp;amp;allocation2%5F1=10&amp;amp;total1=100&amp;amp;endasset1=TotalStockMarket&amp;amp;endallocation1%5F1=30&amp;amp;endasset2=TotalBond&amp;amp;endallocation2%5F1=50&amp;amp;endasset3=ShortTreasury&amp;amp;endallocation3%5F1=20&amp;amp;endtotal1=100&amp;amp;cfname1=Saving&amp;amp;cftype1=1&amp;amp;cfamount1=12000&amp;amp;cfinfadj1=true&amp;amp;%5F%5Fcheckbox%5Fcfinfadj1=true&amp;amp;cfstart1=1&amp;amp;cffrequency1=4&amp;amp;cfoccurs1=1&amp;amp;cfname2=Retirement&amp;amp;cftype2=2&amp;amp;cfamount2=48000&amp;amp;cfinfadj2=true&amp;amp;%5F%5Fcheckbox%5Fcfinfadj2=true&amp;amp;cfstart2=2&amp;amp;cffrequency2=4&amp;amp;cfoccurs2=2&amp;amp;cftype3=2&amp;amp;%5F%5Fcheckbox%5Fcfinfadj3=true&amp;amp;cfstart3=3&amp;amp;cffrequency3=4&amp;amp;cfoccurs3=3&#34;&gt;This simulation&lt;/a&gt; &lt;a href=&#34;WithSS.pdf&#34;&gt;[pdf]&lt;/a&gt;, based on historical data, says that a plan like this would have succeeded about 97% of the time with typical investments, and &lt;a href=&#34;https://www.portfoliovisualizer.com/financial-goals?s=y&amp;amp;stages=2&amp;amp;careerYears=41&amp;amp;mode=1&amp;amp;initialAmount=12000&amp;amp;years=70&amp;amp;simulationModel=1&amp;amp;historicalVolatility=true&amp;amp;fullHistory=true&amp;amp;startYear=1972&amp;amp;endYear=2018&amp;amp;bootstrapModel=1&amp;amp;bootstrapMinYears=1&amp;amp;bootstrapMaxYears=20&amp;amp;circularBootstrap=true&amp;amp;distribution=1&amp;amp;dof=30&amp;amp;meanReturn=7.0&amp;amp;volatility=12.0&amp;amp;sequenceStressTest=0&amp;amp;stressTestRetirement=true&amp;amp;inflationModel=1&amp;amp;inflationMean=4.26&amp;amp;inflationVolatility=3.13&amp;amp;customIntervals=false&amp;amp;percentileList=10%2C+25%2C+50%2C+75%2C+90&amp;amp;returnList=0%2C+2.5%2C+5%2C+7.5%2C+10%2C+12.5&amp;amp;asset1=TotalStockMarket&amp;amp;allocation1%5F1=90&amp;amp;asset2=TotalBond&amp;amp;allocation2%5F1=10&amp;amp;total1=100&amp;amp;endasset1=TotalStockMarket&amp;amp;endallocation1%5F1=30&amp;amp;endasset2=TotalBond&amp;amp;endallocation2%5F1=50&amp;amp;endasset3=ShortTreasury&amp;amp;endallocation3%5F1=20&amp;amp;endtotal1=100&amp;amp;cfname1=Saving&amp;amp;cftype1=1&amp;amp;cfamount1=12000&amp;amp;cfinfadj1=true&amp;amp;%5F%5Fcheckbox%5Fcfinfadj1=true&amp;amp;cfstart1=1&amp;amp;cffrequency1=4&amp;amp;cfoccurs1=1&amp;amp;cfname2=Retirement&amp;amp;cftype2=2&amp;amp;cfamount2=80000&amp;amp;cfinfadj2=true&amp;amp;%5F%5Fcheckbox%5Fcfinfadj2=true&amp;amp;cfstart2=2&amp;amp;cffrequency2=4&amp;amp;cfoccurs2=2&amp;amp;cftype3=2&amp;amp;%5F%5Fcheckbox%5Fcfinfadj3=true&amp;amp;cfstart3=3&amp;amp;cffrequency3=4&amp;amp;cfoccurs3=3&#34;&gt;even lacking social security&lt;/a&gt; &lt;a href=&#34;WithoutSS.pdf&#34;&gt;[pdf]&lt;/a&gt; would have succeeded about 76% of the time. A warning, however: I not a financial expert, so caveat emptor.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The biggest financial problem in anyone’s life is how to provide for oneself in retirement. If you retire in your 60’s, you have a good chance of living for another several decades. Even someone used to living on a modest income of $40,000 a year, could need over half a million dollars saved up to keep from running out of money. &amp;ldquo;Half a million dollars‽&amp;rdquo; you cry. &amp;ldquo;I can barely afford my &lt;a href=&#34;https://www.waterstonegroup.com/insights-and-news/americas-relationship-with-subscription-services/&#34;&gt;monthly subscription services&lt;/a&gt;!&amp;rdquo; Fortunately, by taking advantage of the &lt;a href=&#34;https://www.snopes.com/fact-check/compound-interest/&#34;&gt;most powerful force in the universe&lt;/a&gt; it’s easier than you think, especially if you get started early.&lt;/p&gt;

&lt;h2 id=&#34;the-formula&#34;&gt;The Formula&lt;/h2&gt;

&lt;p&gt;After thinking about it really hard for a while, you come up with the following goal: &amp;ldquo;I want to retire at 67 with enough savings to live for 20 years at 80% of my usual income.&amp;rdquo; What can you do to have a fair chance of meeting this goal? How much you need to save depends most of all on what age you start saving at. Under some reasonable assumptions, the percent \(p\) of your income you would need to save if you started saving at age \(A\) would be:
\[\formbox{p = 12\times0.8\left(\frac{0.03}{1.03^N - 1}\right)}\]
where \(N=67-A\). If you wanted to type it in to a &lt;a href=&#34;https://en.wikipedia.org/wiki/TI-30&#34;&gt;calculator&lt;/a&gt;, the keypresses would likely be something like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-nil&#34; data-lang=&#34;nil&#34;&gt;12 * 0.8 * 0.03 ÷ ( 1.03 ^ N - 1 )&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Or you could use the fancy Javascript calculator &lt;a href=&#34;#appendix-calculator&#34;&gt;below&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;

&lt;p&gt;For instance, say you start saving at 25. Then, assuming no prolonged periods of unemployment, you would be saving for \(N=67-25=42\) years. Plugging this in to the formula, we get&lt;/p&gt;

&lt;p&gt;\[
p = 12 \times 0.8 \left(\frac{0.03}{1.03^{42} - 1}\right) = 0.117
\]&lt;/p&gt;

&lt;p&gt;So you would need to put about 12% of your paycheck into some kind of &lt;a href=&#34;https://investor.vanguard.com/mutual-funds/target-retirement/&#34;&gt;retirement fund&lt;/a&gt; in order to meet your goal. If you make $50,000 a year, 12% of your income comes to $6000 a year, or $500 a month. Many employers, however, will offer to &lt;a href=&#34;https://www.investopedia.com/articles/personal-finance/120315/what-good-401k-match.asp&#34;&gt;match&lt;/a&gt; a portion of your contributions. If your employer matches&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; 3%, then you would only need to contribute the other 9%. At $50,000 a year, this is a contribution of $375 a month.
&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MFM Journal Club: Permitted and Forbidden Sets in STLNs</title>
      <link>https://mathformachines.com/posts/permitted-and-forbidden-sets/</link>
      <pubDate>Mon, 08 Apr 2019 06:47:00 -0500</pubDate>
      
      <guid>https://mathformachines.com/posts/permitted-and-forbidden-sets/</guid>
      <description>&lt;h2 id=&#34;a-model-of-associative-memory&#34;&gt;A Model of Associative Memory&lt;/h2&gt;

&lt;p&gt;How memories are encoded in neural matter is still an open question. The name for these supposed neural correlates of memory is &amp;ldquo;engram&amp;rdquo;, and papers about engrams tend to have titles like &lt;a href=&#34;https://jflab.ca/pdfs/josselyn-et-al-2015.pdf&#34;&gt;&lt;em&gt;Finding the engram&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3462696/&#34;&gt;&lt;em&gt;Catching the engram&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://psycnet.apa.org/record/1952-05966-020&#34;&gt;&lt;em&gt;In search of the engram&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2895151/&#34;&gt;&lt;em&gt;Continuing the search for the engram&lt;/em&gt;&lt;/a&gt;, which, though I&amp;rsquo;m not an expert, makes me feel like the problem isn&amp;rsquo;t well understood.&lt;/p&gt;

&lt;p&gt;(Also, &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/15450162&#34;&gt;&lt;em&gt;Rite of passage of the engram&lt;/em&gt;&lt;/a&gt; and &lt;a href=&#34;http://www.jneurosci.org/content/34/42/14115&#34;&gt;&lt;em&gt;Manipulating a cocaine engram&lt;/em&gt;&lt;/a&gt;, making the practice of neuroscience sometimes sound like a fraternity hazing. Possibly related, while researching this post I learned that a typical experiment will usually involve things like shocking the feet of a fruit fly, sewing shut one eye of a barn owl, and shaving half the whiskers off a mouse.)&lt;/p&gt;

&lt;p&gt;A popular theory is that memories are encoded as patterns of synaptic connections. Perception creates neural activity. Neural activity leaves an impression upon the brain as a pattern of modified synaptic connections (perhaps by &lt;a href=&#34;https://en.wikipedia.org/wiki/Dendritic%5Fspine#Importance%5Fto%5Flearning%5Fand%5Fmemory&#34;&gt;dendritic spines&lt;/a&gt;, which become larger and more numerous to make the connection stronger). A later perception might partly activate this pattern, but this partial activation is often enough to activate the rest of the pattern, too. This is supposed to be a neural model of associative memory. (The tradition is to cite &lt;a href=&#34;https://en.wikipedia.org/wiki/In%5FSearch%5Fof%5FLost%5FTime#Memory&#34;&gt;Proust&lt;/a&gt; at this point; evidently, a &lt;a href=&#34;https://en.wikipedia.org/wiki/Madeleine%5F(cake)&#34;&gt;sponge cake&lt;/a&gt; was sufficient to activate in him the neural substrate of a &lt;a href=&#34;https://en.wikipedia.org/wiki/List%5Fof%5Flongest%5Fnovels&#34;&gt;1,267,069&lt;/a&gt; word novel. It&amp;rsquo;s remarkably illustrative, at least.)&lt;/p&gt;

&lt;p&gt;Artificial neural networks are often used to model the networks of the brain. &lt;a href=&#34;https://en.wikipedia.org/wiki/Feedforward%5Fneural%5Fnetwork&#34;&gt;Feedforward networks&lt;/a&gt; have typically been used to model the visual system, while &lt;a href=&#34;https://en.wikipedia.org/wiki/Recurrent%5Fneural%5Fnetwork&#34;&gt;recurrent networks&lt;/a&gt; have more often been used to model memory. When an input is applied to certain of these recurrent networks, the neural activity will always converge to a stable &lt;a href=&#34;https://en.wikipedia.org/wiki/Steady%5Fstate&#34;&gt;steady state&lt;/a&gt;. This stable pattern of activity is supposed to be a memory, stored within the connections of the network.&lt;/p&gt;

&lt;p&gt;Some of the most studied networks are those that are &lt;em&gt;symmetrically&lt;/em&gt; connected, like the &lt;a href=&#34;https://en.wikipedia.org/wiki/Hopfield%5Fnetwork&#34;&gt;Hopfield network&lt;/a&gt;. A network is symmetrically connected if every neuron is connected with the same weight as whatever is connected to it. A symmetrically connected network with a &lt;em&gt;linear&lt;/em&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Activation%5Ffunction&#34;&gt;activation function&lt;/a&gt; can, for a given set of connection weights, be activated only to a &lt;em&gt;single&lt;/em&gt; stable steady state (whose values depend upon the input to the network). The drawback of these networks then is that the activity at future states will be independent of the activity at past states. Past recall cannot influence future recall.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://papers.nips.cc/paper/1793-permitted-and-forbidden-sets-in-symmetric-threshold-linear-networks.pdf&#34;&gt;Hahnloser and Seung&lt;/a&gt; present a model of associative memory in symmetrically connected networks using instead a &lt;em&gt;threshold-linear&lt;/em&gt; activation function (or &lt;a href=&#34;https://en.wikipedia.org/wiki/Rectifier%5F(neural%5Fnetworks)&#34;&gt;rectified linear&lt;/a&gt; function). 


&lt;figure class=&#34;floatright&#34;&gt;
    
        &lt;img src=&#34;https://mathformachines.com/posts/permitted-and-forbidden-sets/rectifier.png&#34; alt=&#34;Graph of a rectified linear activation function.&#34;/&gt; &lt;/figure&gt;
 They show that, due to some nice properties of the rectifier, such networks can in general represent multiple patterns of stable activation even for a single input. What pattern the network will fall into upon new input, depends upon what pattern it was in before. Memories linger.&lt;/p&gt;

&lt;p&gt;Their main contribution in this paper is in classifying neurons into what they call &amp;ldquo;permitted&amp;rdquo; and &amp;ldquo;forbidden&amp;rdquo; sets, which describe what sets of neurons may be activated together in a stable steady-state. They describe a method of determining what patterns of stable activity the network can achieve.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The existence of permitted and forbidden sets suggests a new way of thinking about
memory in neural networks. When an input is applied, the network must select a set
of active neurons, and this selection is constrained to be one of the permitted sets.
Therefore the permitted sets can be regarded as memories stored in the synaptic
connections.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Change of Basis for Vectors and Covectors</title>
      <link>https://mathformachines.com/posts/change-of-basis/</link>
      <pubDate>Mon, 18 Mar 2019 07:28:00 -0500</pubDate>
      
      <guid>https://mathformachines.com/posts/change-of-basis/</guid>
      <description>&lt;blockquote&gt;
  &lt;p&gt;We share a philosophy about linear algebra: we think basis-free, we write basis-free, but when the chips are down we close the office door and compute with matrices like fury.&lt;/p&gt;
  &lt;footer&gt;
    &lt;strong&gt;- Irving Kaplansky&lt;/strong&gt;
    
      
        &lt;cite&gt;
          &lt;a href=&#34;#ZgotmplZ&#34; title=&#34;&amp;lt;https://mathoverflow.net/questions/11669/what-is-the-difference-between-matrix-theory-and-linear-algebra/19923&amp;gt;&#34;&gt;mathoverflow.net/questions/...&lt;/a&gt; 
        &lt;/cite&gt;
      
    
  &lt;/footer&gt;
&lt;/blockquote&gt;


&lt;p&gt;Often, the first step in analyzing a problem is to &lt;em&gt;transform&lt;/em&gt; it into something more amenable to our analysis. We would like the &lt;em&gt;representation&lt;/em&gt; of our problem to reflect as naturally as possible whatever features of it we are most interested in. We might normalize data through a scaling transform, for instance, to eliminate spurious differences among like quantities. Or we might rotate data to align some of its salient dimensions with the coordinate axes, simplifying computations. Many matrix decompositions take the form \(M = BNA\). When \(B\) and \(A\) are non-singular, we can think of \(N\) as being a simpler representation of \(M\) under coordinate transforms \(B\) and \(A\). The &lt;a href=&#34;https://en.wikipedia.org/wiki/Eigendecomposition%5Fof%5Fa%5Fmatrix&#34;&gt;spectral decomposition&lt;/a&gt; and the &lt;a href=&#34;https://en.wikipedia.org/wiki/Singular%5Fvalue%5Fdecomposition&#34;&gt;singular value decomposition&lt;/a&gt; are of this form.&lt;/p&gt;

&lt;p&gt;All of these kinds of coordinate transformations are &lt;em&gt;linear&lt;/em&gt; transformations. Linear coordinate transformations come about from operations on basis vectors that leave any vectors represented by them &lt;a href=&#34;https://en.wikipedia.org/wiki/Active%5Fand%5Fpassive%5Ftransformation&#34;&gt;unchanged&lt;/a&gt;. They are, in other words, a change of basis.&lt;/p&gt;

&lt;p&gt;This post came about from my frustration at not finding simple formulas for these transformations with simple explanations to go along with them. So here, I tried to give a simple exposition of coordinate transformations for vectors in vector spaces along with transformations of their cousins, the covectors in the dual space. I&amp;rsquo;ll get into matrices and some applications in a future post.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A Tour of Tensors</title>
      <link>https://mathformachines.com/posts/a-tour-of-tensors/</link>
      <pubDate>Tue, 05 Feb 2019 12:59:00 -0600</pubDate>
      
      <guid>https://mathformachines.com/posts/a-tour-of-tensors/</guid>
      <description>&lt;p&gt;Tensors can sometimes have a fearsome reputation. They are at heart, however, no more difficult to define than polynomials. I&amp;rsquo;ve tried in these notes to take a computational focus and to avoid formalism when possible; I haven&amp;rsquo;t assumed any more than what you might encounter in an undergraduate linear algebra course. If you&amp;rsquo;re interested in tensors applied to machine learning, or have wondered why arrays in Tensorflow are called tensors, you might find this useful. I&amp;rsquo;ll do some computations in &lt;a href=&#34;http://www.sagemath.org/&#34;&gt;Sage&lt;/a&gt; and also in &lt;a href=&#34;http://www.numpy.org/&#34;&gt;Numpy&lt;/a&gt; for illustration.&lt;/p&gt;

&lt;h2 id=&#34;abstract-tensors&#34;&gt;Abstract Tensors&lt;/h2&gt;

&lt;p&gt;First, let&amp;rsquo;s take brief look at tensors in the abstract. This is just to give us an idea of what properties they have and how they function. I&amp;rsquo;ll gloss over most of the details of the construction.&lt;/p&gt;

&lt;p&gt;A tensor is a vector. It is an element of a vector space. Being a vector, if we have a basis for the space we can write the tensor as a list of coordinates (or maybe something like a matrix or an array – we&amp;rsquo;ll see how).&lt;/p&gt;

&lt;p&gt;A tensor is a vector in a product vector space. This means that part of it comes from one vector space and part of it comes from another. These parts combine in a way that fits with the usual notions of how products should work. Why would we want these tensors, these products of vectors? It turns out that lots of useful things are tensors. Matrices and linear maps are tensors, and so are determinants and inner products and cross products. Tensors give us power to express many useful ideas.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Bayesian Topic Modeling</title>
      <link>https://mathformachines.com/posts/bayesian-topic-modeling/</link>
      <pubDate>Wed, 30 Jan 2019 05:57:00 -0600</pubDate>
      
      <guid>https://mathformachines.com/posts/bayesian-topic-modeling/</guid>
      <description>&lt;p&gt;Imagine we have some collection of documents. They could be novels, or tweets, or financial reports&amp;mdash;just any collection of text. We want an algorithm that can discover what they are about, and we would like our algorithm to do it automatically, without any hints. (That is, we want our algorithm to be &lt;em&gt;unsupervised&lt;/em&gt;.) We will look at several models that probabilistically assign words to topics using &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayes&#39;%5Ftheorem&#34;&gt;Bayes&amp;rsquo; Theorem&lt;/a&gt;. They are all &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayesian%5Fnetwork&#34;&gt;Bayesian Graphical Models&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The basic problem in statistics is to infer some unobservable value from observable instances of it. In our case, we want to infer the &lt;em&gt;topics&lt;/em&gt; of a document from the actual words in the document. We want to be able to infer that our document is about &amp;ldquo;colors&amp;rdquo; if we observe &amp;ldquo;red&amp;rdquo; and &amp;ldquo;green&amp;rdquo; and &amp;ldquo;blue&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Bayes&amp;rsquo; Theorem allows us to do this. It allows us to infer probabilities concerning the unobserved value from the observations that we can make. It allows us to reason backwards in some sense. So, when constructing a Bayesian model, it is helpful to &lt;em&gt;think&lt;/em&gt; backwards. Instead of first asking how words are distributed to topics and topics to documents, we will ask how we could &lt;em&gt;generate&lt;/em&gt; a document if we already knew these distributions. To construct our model, we will first reason from the unknown values to the known values so that we know how to do the converse when the time comes.
&lt;/p&gt;</description>
    </item>
    
    <item>
      <title></title>
      <link>https://mathformachines.com/pages/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mathformachines.com/pages/about/</guid>
      <description>This is the website of Ryan Holbrook. I&amp;rsquo;m looking for work. You can contact me at ryan@mathformachines.com. Twitter
Github</description>
    </item>
    
  </channel>
</rss>
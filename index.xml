<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Math for Machines</title>
    <link>//mathformachines.com/</link>
    <description>Recent content on Math for Machines</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy;Ryan Holbrook 2019</copyright>
    <lastBuildDate>Tue, 05 Feb 2019 12:59:00 -0600</lastBuildDate>
    
	<atom:link href="//mathformachines.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A Tour of Tensors</title>
      <link>//mathformachines.com/posts/a-tour-of-tensors/</link>
      <pubDate>Tue, 05 Feb 2019 12:59:00 -0600</pubDate>
      
      <guid>//mathformachines.com/posts/a-tour-of-tensors/</guid>
      <description>Here are some notes on tensors, starting from elementary linear algebra. I&amp;rsquo;ve tried to take a computational focus and to avoid formalism when possible. If you&amp;rsquo;re interested in tensors applied to machine learning, or have wondered why arrays in Tensorflow are called tensors, you might find this useful. I&amp;rsquo;ll do some computations in Sage and also in Numpy for illustration.
Abstract Tensors First, let&amp;rsquo;s take brief look at tensors in the abstract.</description>
    </item>
    
    <item>
      <title>Bayesian Topic Modeling</title>
      <link>//mathformachines.com/posts/bayesian-topic-modeling/</link>
      <pubDate>Wed, 30 Jan 2019 05:57:00 -0600</pubDate>
      
      <guid>//mathformachines.com/posts/bayesian-topic-modeling/</guid>
      <description>Imagine we have some collection of documents. They could be novels, or tweets, or financial reports&amp;mdash;just any collection of text. We want an algorithm that can discover what they are about, and we would like our algorithm to do it automatically, without any hints. (That is, we want our algorithm to be unsupervised.) We will look at several models that probabilistically assign words to topics using Bayes&amp;rsquo; Theorem. They are all Bayesian Graphical Models.</description>
    </item>
    
    <item>
      <title></title>
      <link>//mathformachines.com/pages/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//mathformachines.com/pages/about/</guid>
      <description>This is the website of Ryan Holbrook. You can contact me at ryan@mathformachines.com.</description>
    </item>
    
  </channel>
</rss>
<!DOCTYPE html>
<html>
  <head>
    
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-133546767-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
      
  Bayesian Topic Modeling &ndash; Math for Machines

    </title>
    
    
    <meta name="description" property="og:description" content="Imagine we have some collection of documents. They could be novels, or tweets, or financial reports&amp;mdash;just any collection of text. We want an algorithm that can discover what they are about, and we would like our algorithm to do it automatically, without any hints. (That is, we want our algorithm to be unsupervised.) We will look at several models that probabilistically assign words to topics using Bayes&amp;rsquo; Theorem. They are all Bayesian Graphical Models.|It&#39;s about math, for machines.">
    

    <meta name="apple-mobile-web-app-title" content="Math for Machines">
    
    
    
    


    <link rel="stylesheet" href="/assets/syntax.css">
    <link rel="stylesheet" href="/assets/primer-build.css">
    <link rel="stylesheet" href="/assets/style.css">
  </head>


  <body class="bg-gray">
    <div id="holy" class="container-lg bg-white h-100">

      <div id="header" class="px-1 bg-white">
        <nav class="UnderlineNav UnderlineNav--right px-2">
  <a class="UnderlineNav-actions text-gray h1" href="//mathformachines.com/">
    Math for Machines
  </a>

  
  
  <div class="UnderlineNav-body text-bold">
    
    
    <a class="UnderlineNav-item " href="/post">
      
      <span>Archive</span>
    </a>
    
    
  </div>
  
</nav>

      </div>

      <div role="main" id="main" class="holy-main markdown-body px-4 bg-white">
        

<div class="Subhead">
  <div class="Subhead-heading">
    <div class="h1 mt-3 mb-1">Bayesian Topic Modeling</div>
  </div>
  <div class="Subhead-description">
    




<a href='/tags/mcmc' class="muted-link">
  <span class="Label Label--gray">MCMC</span>
</a>

<a href='/tags/nlp' class="muted-link">
  <span class="Label Label--gray">NLP</span>
</a>

<a href='/tags/bayesian' class="muted-link">
  <span class="Label Label--gray">bayesian</span>
</a>



    
    <div class="float-md-right">
      <span title="Last Modified: 2019-02-05. Published at: 2019-01-30.">
        
          Lastmod: 2019-02-05
        
      </span>
    </div>
    
  </div>
</div>
<article>
  
  <section class="pb-6 mb-3 border-bottom">
    

<p>Imagine we have some collection of documents. They could be novels, or tweets, or financial reports&mdash;just any collection of text. We want an algorithm that can discover what they are about, and we would like our algorithm to do it automatically, without any hints. (That is, we want our algorithm to be <em>unsupervised</em>.) We will look at several models that probabilistically assign words to topics using <a href="https://en.wikipedia.org/wiki/Bayes%27%5Ftheorem">Bayes&rsquo; Theorem</a>. They are all <a href="https://en.wikipedia.org/wiki/Bayesian%5Fnetwork">Bayesian Graphical Models</a>.</p>

<p>The basic problem in statistics is to infer some unobservable value from observable instances of it. In our case, we want to infer the <em>topics</em> of a document from the actual words in the document. We want to be able to infer that our document is about &ldquo;colors&rdquo; if we observe &ldquo;red&rdquo; and &ldquo;green&rdquo; and &ldquo;blue&rdquo;.</p>

<p>Bayes&rsquo; Theorem allows us to do this. It allows us to infer probabilities concerning the unobserved value from the observations that we can make. It allows us to reason backwards in some sense. So, when constructing a Bayesian model, it is helpful to <em>think</em> backwards. Instead of first asking how words are distributed to topics and topics to documents, we will ask how we could <em>generate</em> a document if we already knew these distributions. To construct our model, we will first reason from the unknown values to the known values so that we know how to do the converse when the time comes.</p>

<h2 id="some-simple-generative-examples">Some Simple Generative Examples</h2>

<p>In all of our models, we are going make a simplfying assumption. We will assume that all of the words in a document occur independently of whatever words came before or come after; that is, a document will just be a &ldquo;bag of words.&rdquo; We&rsquo;ll see that even with ignoring word-order, we can still infer pretty accurately what a document might be about.</p>

<p>Let&rsquo;s start with a very simple example: 1 document with 1 topic and 2 words in our vocabulary.</p>

<p>(Some definitions: The &ldquo;vocabulary&rdquo; is just the set of unique words that occur in all of the documents together, the &ldquo;corpus.&rdquo; We&rsquo;ll refer to a word in the vocabulary as just a &ldquo;word&rdquo; and some instance of a word in a document as a &ldquo;token.&rdquo;)</p>

<p>Let&rsquo;s say our two words are &ldquo;blue&rdquo; and &ldquo;red&rdquo;, and that the probability of any given word (any token) being &ldquo;red&rdquo; is \(\phi\): \(P(W = red) = \phi\). This is the same as saying our random variable of tokens \(W\) has a Bernoulli distribution with parameter \(\phi\): \(W \sim Bernoulli(\phi)\).</p>

<p>The distribution looks like this:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3">x <span style="color:#719e07">=</span> [<span style="color:#2aa198">0</span>, <span style="color:#2aa198">1</span>]
pmf <span style="color:#719e07">=</span> st<span style="color:#719e07">.</span>bernoulli<span style="color:#719e07">.</span>pmf(x, <span style="color:#2aa198">0.3</span>)

plt<span style="color:#719e07">.</span>stem(x, pmf)
plt<span style="color:#719e07">.</span>xticks([<span style="color:#2aa198">0</span>,<span style="color:#2aa198">1</span>])
plt<span style="color:#719e07">.</span>ylim(<span style="color:#2aa198">0</span>,<span style="color:#2aa198">1</span>)
plt<span style="color:#719e07">.</span>xlim(<span style="color:#719e07">-</span><span style="color:#2aa198">0.5</span>, <span style="color:#2aa198">1.5</span>)</code></pre></div>



<figure>
    
        <img src="//mathformachines.com/post/bayesian-topic-modeling/obipy-resources/14366Vwu.png"/> </figure>


<p>Here, 1 represents &ldquo;red&rdquo; and 0 represents &ldquo;blue&rdquo; (or not-&ldquo;red&rdquo;).</p>

<p>And here is how we could generate a document with this model:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3">coding <span style="color:#719e07">=</span> {<span style="color:#2aa198">0</span> : <span style="color:#2aa198">&#34;blue&#34;</span>, <span style="color:#2aa198">1</span> : <span style="color:#2aa198">&#34;red&#34;</span>}
W <span style="color:#719e07">=</span> <span style="color:#2aa198">50</span>  <span style="color:#586e75"># number of tokens in the document</span>
tokens <span style="color:#719e07">=</span> st<span style="color:#719e07">.</span>bernoulli<span style="color:#719e07">.</span>rvs(<span style="color:#2aa198">0.3</span>, size <span style="color:#719e07">=</span> W)  <span style="color:#586e75"># choose the tokens</span>
<span style="color:#b58900">print</span>(<span style="color:#2aa198">&#39; &#39;</span><span style="color:#719e07">.</span>join(<span style="color:#b58900">str</span>(w) <span style="color:#719e07">for</span> w <span style="color:#719e07">in</span> [coding[i] <span style="color:#719e07">for</span> i <span style="color:#719e07">in</span> tokens]))</code></pre></div><div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">blue blue blue red blue red red red red blue blue blue blue blue red blue blue blue red blue blue blue blue red blue red red red red blue blue blue red blue blue blue blue red blue red blue blue blue blue blue red blue blue blue red</code></pre></div>
<h3 id="unigram-model">Unigram Model</h3>

<p>For the general model, we will also choose the distribution of words within the topic randomly. That is, we will assign a probability distribution to \(\phi\).</p>

<p>The <a href="https://en.wikipedia.org/wiki/Beta%5Fdistribution">beta distribution</a> is a natural choice. Since its support is \([0,1]\) it can represent randomly chosen probabilities (values between 0 and 1). It is also conceptually convenient being the <a href="https://en.wikipedia.org/wiki/Conjugate%5Fprior">conjugate prior</a> of the Bernoulli distribution, which allows us to make a more explicit connection between its parameters and the parameters of its Bernoulli distribution.</p>

<p>The model is now:</p>

<p>\(\phi \sim Beta(\beta_0, \beta_1)\)</p>

<p>\(W \sim Bernoulli(\phi)\)</p>

<p>where \(\beta_0\) and \(\beta_1\) are the &ldquo;shape parameters&rdquo; of the beta distribution. We can think of them as the assumed counts of each word, or the &ldquo;pseudo-counts.&rdquo; Let&rsquo;s see how different values of these parameters affect the shape of the distribution.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3">beta_0 <span style="color:#719e07">=</span> [<span style="color:#2aa198">0.8</span>, <span style="color:#2aa198">1</span>, <span style="color:#2aa198">2</span>, <span style="color:#2aa198">10</span>]
beta_1 <span style="color:#719e07">=</span> [<span style="color:#2aa198">0.8</span>, <span style="color:#2aa198">1</span>, <span style="color:#2aa198">2</span>, <span style="color:#2aa198">10</span>]

x <span style="color:#719e07">=</span> np<span style="color:#719e07">.</span>array(np<span style="color:#719e07">.</span>linspace(<span style="color:#2aa198">0</span>, <span style="color:#2aa198">1</span>, <span style="color:#2aa198">1000</span>))

f, axarr <span style="color:#719e07">=</span> plt<span style="color:#719e07">.</span>subplots(<span style="color:#b58900">len</span>(beta_0), <span style="color:#b58900">len</span>(beta_1), sharex<span style="color:#719e07">=</span><span style="color:#2aa198">&#39;all&#39;</span>, sharey<span style="color:#719e07">=</span><span style="color:#2aa198">&#39;none&#39;</span>)

<span style="color:#719e07">for</span> i <span style="color:#719e07">in</span> <span style="color:#b58900">range</span>(<span style="color:#b58900">len</span>(beta_0)):
    <span style="color:#719e07">for</span> j <span style="color:#719e07">in</span> <span style="color:#b58900">range</span>(<span style="color:#b58900">len</span>(beta_1)):
        a <span style="color:#719e07">=</span> beta_0[i]
        b <span style="color:#719e07">=</span> beta_1[j]
        y <span style="color:#719e07">=</span> st<span style="color:#719e07">.</span>beta(a, b)<span style="color:#719e07">.</span>pdf(x)
        axarr[i, j]<span style="color:#719e07">.</span>plot(x, y)
        axarr[i, j]<span style="color:#719e07">.</span>axes<span style="color:#719e07">.</span>yaxis<span style="color:#719e07">.</span>set_ticklabels([])
        axarr[i, j]<span style="color:#719e07">.</span>set_title(<span style="color:#2aa198">r</span><span style="color:#2aa198">&#39;$\beta_0 =$ &#39;</span> <span style="color:#719e07">+</span> <span style="color:#b58900">str</span>(a) <span style="color:#719e07">+</span> <span style="color:#2aa198">r</span><span style="color:#2aa198">&#39;, $\beta_1 =$ &#39;</span> <span style="color:#719e07">+</span> <span style="color:#b58900">str</span>(b))

f<span style="color:#719e07">.</span>subplots_adjust(hspace<span style="color:#719e07">=</span><span style="color:#2aa198">0.3</span>)
f<span style="color:#719e07">.</span>suptitle(<span style="color:#2aa198">r</span><span style="color:#2aa198">&#39;Beta Distributions for $\theta$&#39;</span>, fontsize<span style="color:#719e07">=</span><span style="color:#2aa198">20</span>)</code></pre></div>



<figure>
    
        <img src="//mathformachines.com/post/bayesian-topic-modeling/obipy-resources/14366i60.png"/> </figure>


<p>Values near 0 will favor &ldquo;blue&rdquo; and values near 1 will favor &ldquo;red&rdquo;. We can choose \(\beta_0\) and \(\beta_1\) to generate the kinds of documents we like. (The notation is a bit backwards here: \(\beta_0\) is the <strong>pseudo-count</strong> for &ldquo;red&rdquo;, whose probability is toward 1, on the right of the graph. So \(\beta_0 &gt; \beta_1\) means more &ldquo;red&rdquo;s, and vice versa.)</p>

<p>Let&rsquo;s generate some documents with this expanded model. We&rsquo;ll set \(\beta_0 = 0.8\) and \(\beta_1 = 0.8\). We would expect most of our documents to favor one word or the other, but overall to occur equally often.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3">beta_0 <span style="color:#719e07">=</span> <span style="color:#2aa198">0.8</span>
beta_1 <span style="color:#719e07">=</span> <span style="color:#2aa198">0.8</span>

thetas <span style="color:#719e07">=</span> st<span style="color:#719e07">.</span>beta<span style="color:#719e07">.</span>rvs(beta_0, beta_1, size <span style="color:#719e07">=</span> <span style="color:#2aa198">6</span>)

W <span style="color:#719e07">=</span> <span style="color:#2aa198">10</span>  <span style="color:#586e75"># number of tokens in each document</span>

<span style="color:#719e07">for</span> t <span style="color:#719e07">in</span> thetas:
    <span style="color:#b58900">print</span>(<span style="color:#2aa198">&#39;Theta: &#39;</span>, t)
    tokens <span style="color:#719e07">=</span> st<span style="color:#719e07">.</span>bernoulli<span style="color:#719e07">.</span>rvs(t, size <span style="color:#719e07">=</span> W)
    <span style="color:#b58900">print</span>(<span style="color:#2aa198">&#39;Document: &#39;</span> <span style="color:#719e07">+</span> <span style="color:#2aa198">&#39; &#39;</span><span style="color:#719e07">.</span>join(<span style="color:#b58900">str</span>(w) <span style="color:#719e07">for</span> w <span style="color:#719e07">in</span> [coding[i] <span style="color:#719e07">for</span> i <span style="color:#719e07">in</span> tokens]) <span style="color:#719e07">+</span> <span style="color:#2aa198">&#39;</span><span style="color:#cb4b16">\n</span><span style="color:#2aa198">&#39;</span>)</code></pre></div><div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">Theta:  0.9302363695290435
Document: red red red red blue red red red red red

Theta:  0.11860480575148062
Document: blue blue blue blue red blue blue blue blue blue

Theta:  0.6974233440265667
Document: blue red blue blue red red red blue red blue

Theta:  0.8751016129026872
Document: red red red red red red red red red red

Theta:  0.49876849418291125
Document: red blue red blue red red red red red red

Theta:  0.18083582294749095
Document: blue red blue blue red red red blue red blue</code></pre></div>
<p>(We could also assign a distribution to W, the number of tokens in each document. (Blei 2003) uses a Poisson distribution.)</p>

<p>Let&rsquo;s look at a couple more.</p>

<h3 id="mixture-of-unigrams">Mixture of Unigrams</h3>

<p>Here, we&rsquo;ll also choose a single topic for each document, from among two. To simplify things, we&rsquo;ll also assume the topics generate distinct words and that the proportions of words in topics are similar, that is, that they have the same shape parameters. We&rsquo;ll see later that is a good assumption when using inference models.</p>

<p>Distribution of topics to documents: \(\theta \sim Beta(\alpha_0, \alpha_1)\)</p>

<p>Distribution of words to Topic 0: \(\phi_0 \sim Beta(\beta_0, \beta_1)\)</p>

<p>Distribution of words to Topic 1: \(\phi_1 \sim Beta(\beta_0, \beta_1)\)</p>

<p>The topics: \(T \sim Bernoulli(\theta)\)</p>

<p>Words from Topic 0: \(W_1 \sim Bernoulli(\phi_0)\)</p>

<p>Words from Topic 1: \(W_2 \sim Bernoulli(\phi_1)\)</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3">coding_0 <span style="color:#719e07">=</span> {<span style="color:#2aa198">0</span>:<span style="color:#2aa198">&#39;blue&#39;</span>, <span style="color:#2aa198">1</span>:<span style="color:#2aa198">&#39;red&#39;</span>}  <span style="color:#586e75"># words in topic 0</span>
coding_1 <span style="color:#719e07">=</span> {<span style="color:#2aa198">0</span>:<span style="color:#2aa198">&#39;dogs&#39;</span>, <span style="color:#2aa198">1</span>:<span style="color:#2aa198">&#39;cats&#39;</span>}  <span style="color:#586e75"># words in topic 1</span>

D <span style="color:#719e07">=</span> <span style="color:#2aa198">15</span>  <span style="color:#586e75"># number of documents in corpus</span>
W <span style="color:#719e07">=</span> <span style="color:#2aa198">10</span>  <span style="color:#586e75"># number of tokens in each document</span>

alpha_0, alpha_1 <span style="color:#719e07">=</span> <span style="color:#2aa198">1</span>, <span style="color:#2aa198">1.5</span>
beta_0, beta_1 <span style="color:#719e07">=</span> <span style="color:#2aa198">0.8</span>, <span style="color:#2aa198">0.8</span>

theta <span style="color:#719e07">=</span> st<span style="color:#719e07">.</span>beta<span style="color:#719e07">.</span>rvs(alpha_0, alpha_1, size <span style="color:#719e07">=</span> <span style="color:#2aa198">1</span>)[<span style="color:#2aa198">0</span>]  <span style="color:#586e75"># choose a distribution of topics to documents</span>
phi_0 <span style="color:#719e07">=</span> st<span style="color:#719e07">.</span>beta<span style="color:#719e07">.</span>rvs(beta_0, beta_1, size <span style="color:#719e07">=</span> <span style="color:#2aa198">1</span>)[<span style="color:#2aa198">0</span>] <span style="color:#586e75"># choose distribution of words in topic 0</span>
phi_1 <span style="color:#719e07">=</span> st<span style="color:#719e07">.</span>beta<span style="color:#719e07">.</span>rvs(beta_0, beta_1, size <span style="color:#719e07">=</span> <span style="color:#2aa198">1</span>)[<span style="color:#2aa198">0</span>] <span style="color:#586e75"># choose distribution of words in topic 1</span>

topics <span style="color:#719e07">=</span> st<span style="color:#719e07">.</span>bernoulli<span style="color:#719e07">.</span>rvs(theta, size <span style="color:#719e07">=</span> D)  <span style="color:#586e75"># choose a topic for each document</span>

<span style="color:#b58900">print</span>(<span style="color:#2aa198">&#39;Theta: </span><span style="color:#2aa198">{:.3f}</span><span style="color:#2aa198">  Phi_0: </span><span style="color:#2aa198">{:.3f}</span><span style="color:#2aa198">  Phi_1: </span><span style="color:#2aa198">{:.3f}</span><span style="color:#2aa198">&#39;</span><span style="color:#719e07">.</span><span style="color:#b58900">format</span>(theta, phi_0, phi_1))
<span style="color:#719e07">for</span> i <span style="color:#719e07">in</span> <span style="color:#b58900">range</span>(D):
    <span style="color:#719e07">if</span> topics[i] <span style="color:#719e07">==</span> <span style="color:#2aa198">0</span>:
        tokens <span style="color:#719e07">=</span> st<span style="color:#719e07">.</span>bernoulli<span style="color:#719e07">.</span>rvs(phi_0, size <span style="color:#719e07">=</span> W)
        <span style="color:#b58900">print</span>(<span style="color:#2aa198">&#39;Document: &#39;</span> <span style="color:#719e07">+</span> <span style="color:#2aa198">&#39; &#39;</span><span style="color:#719e07">.</span>join(<span style="color:#b58900">str</span>(w)
              <span style="color:#719e07">for</span> w <span style="color:#719e07">in</span> [coding_0[i] <span style="color:#719e07">for</span> i <span style="color:#719e07">in</span> tokens]))
    <span style="color:#719e07">else</span>:
        tokens <span style="color:#719e07">=</span> st<span style="color:#719e07">.</span>bernoulli<span style="color:#719e07">.</span>rvs(phi_1, size <span style="color:#719e07">=</span> W)
        <span style="color:#b58900">print</span>(<span style="color:#2aa198">&#39;Document: &#39;</span> <span style="color:#719e07">+</span> <span style="color:#2aa198">&#39; &#39;</span><span style="color:#719e07">.</span>join(<span style="color:#b58900">str</span>(w)
              <span style="color:#719e07">for</span> w <span style="color:#719e07">in</span> [coding_1[i] <span style="color:#719e07">for</span> i <span style="color:#719e07">in</span> tokens]))</code></pre></div><div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">Theta: 0.040  Phi_0: 0.574  Phi_1: 0.431
Document: red blue red blue blue blue blue blue red red
Document: blue blue red red blue blue blue blue red blue
Document: blue red blue red red red red blue blue red
Document: red blue blue blue blue red red blue red blue
Document: blue red red red red red blue blue blue red
Document: red blue blue red blue red red red blue red
Document: red blue red blue red blue red red blue blue
Document: red blue red red blue blue blue blue red red
Document: red blue red blue blue red blue red blue red
Document: cats cats dogs dogs cats cats cats dogs cats dogs
Document: blue red blue blue blue red red red blue blue
Document: blue red red red blue red blue red red red
Document: red red blue blue red blue red blue blue blue
Document: blue red blue red blue red blue red red red
Document: blue red red blue red red blue red blue blue</code></pre></div>
<h3 id="latent-dirichlet-allocation">Latent Dirichlet Allocation</h3>

<p>This time, instead of choosing a single topic for each document, we&rsquo;ll choose a topic for each word. This will make our model much more flexible and its behavior more realistic.</p>

<p>Distribution of topics <strong>within</strong> documents: \(\theta \sim Beta(\alpha_0, \alpha_1)\)</p>

<p>Distribution of words to Topic 0: \(\phi_0 \sim Beta(\beta_0, \beta_1)\)</p>

<p>Distribution of words to Topic 1: \(\phi_1 \sim Beta(\beta_0, \beta_1)\)</p>

<p>The topics: \(T \sim Bernoulli(\theta)\)</p>

<p>Words from Topic 0: \(W_1 \sim Bernoulli(\phi_0)\)</p>

<p>Words from Topic 1: \(W_2 \sim Bernoulli(\phi_1)\)</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3">coding_0 <span style="color:#719e07">=</span> {<span style="color:#2aa198">0</span>:<span style="color:#2aa198">&#39;blue&#39;</span>, <span style="color:#2aa198">1</span>:<span style="color:#2aa198">&#39;red&#39;</span>}  <span style="color:#586e75"># words in topic 0</span>
coding_1 <span style="color:#719e07">=</span> {<span style="color:#2aa198">0</span>:<span style="color:#2aa198">&#39;dogs&#39;</span>, <span style="color:#2aa198">1</span>:<span style="color:#2aa198">&#39;cats&#39;</span>}  <span style="color:#586e75"># words in topic 1</span>

D <span style="color:#719e07">=</span> <span style="color:#2aa198">15</span>
W <span style="color:#719e07">=</span> <span style="color:#2aa198">10</span>  <span style="color:#586e75"># number of tokens in each document</span>

alpha_0, alpha_1 <span style="color:#719e07">=</span> <span style="color:#2aa198">1</span>, <span style="color:#2aa198">1.5</span>
beta_0, beta_1 <span style="color:#719e07">=</span> <span style="color:#2aa198">0.8</span>, <span style="color:#2aa198">0.8</span>

theta <span style="color:#719e07">=</span> st<span style="color:#719e07">.</span>beta<span style="color:#719e07">.</span>rvs(alpha_0, alpha_1, size <span style="color:#719e07">=</span> <span style="color:#2aa198">1</span>)[<span style="color:#2aa198">0</span>]  <span style="color:#586e75"># choose a distribution of topics to documents</span>
phi_0 <span style="color:#719e07">=</span> st<span style="color:#719e07">.</span>beta<span style="color:#719e07">.</span>rvs(beta_0, beta_1, size <span style="color:#719e07">=</span> <span style="color:#2aa198">1</span>)[<span style="color:#2aa198">0</span>]  <span style="color:#586e75"># choose distribution of words in topic 0</span>
phi_1 <span style="color:#719e07">=</span> st<span style="color:#719e07">.</span>beta<span style="color:#719e07">.</span>rvs(beta_0, beta_1, size <span style="color:#719e07">=</span> <span style="color:#2aa198">1</span>)[<span style="color:#2aa198">0</span>]  <span style="color:#586e75"># choose distribution of words in topic 1</span>

<span style="color:#b58900">print</span>(<span style="color:#2aa198">&#39;Theta: </span><span style="color:#2aa198">{:.3f}</span><span style="color:#2aa198">  Phi_0: </span><span style="color:#2aa198">{:.3f}</span><span style="color:#2aa198">  Phi_1: </span><span style="color:#2aa198">{:.3f}</span><span style="color:#2aa198">&#39;</span><span style="color:#719e07">.</span><span style="color:#b58900">format</span>(theta, phi_0, phi_1))
<span style="color:#719e07">for</span> i <span style="color:#719e07">in</span> <span style="color:#b58900">range</span>(D):
    <span style="color:#b58900">print</span>(<span style="color:#2aa198">&#39;Document: &#39;</span>, end<span style="color:#719e07">=</span><span style="color:#2aa198">&#39;&#39;</span>)
    topics <span style="color:#719e07">=</span> st<span style="color:#719e07">.</span>bernoulli<span style="color:#719e07">.</span>rvs(theta, size<span style="color:#719e07">=</span>W)  <span style="color:#586e75"># choose topics for each word</span>
    <span style="color:#719e07">for</span> j <span style="color:#719e07">in</span> <span style="color:#b58900">range</span>(W):
        <span style="color:#719e07">if</span> topics[j] <span style="color:#719e07">==</span> <span style="color:#2aa198">0</span>:
            token <span style="color:#719e07">=</span> st<span style="color:#719e07">.</span>bernoulli<span style="color:#719e07">.</span>rvs(phi_0, size<span style="color:#719e07">=</span><span style="color:#2aa198">1</span>)[<span style="color:#2aa198">0</span>]  <span style="color:#586e75"># choose a word from topic 0</span>
            <span style="color:#b58900">print</span>(coding_0[token], end<span style="color:#719e07">=</span><span style="color:#2aa198">&#39; &#39;</span>)
        <span style="color:#719e07">else</span>:
            token <span style="color:#719e07">=</span> st<span style="color:#719e07">.</span>bernoulli<span style="color:#719e07">.</span>rvs(phi_1, size<span style="color:#719e07">=</span><span style="color:#2aa198">1</span>)[<span style="color:#2aa198">0</span>]  <span style="color:#586e75"># choose a word from topic 1</span>
            <span style="color:#b58900">print</span>(coding_1[token], end<span style="color:#719e07">=</span><span style="color:#2aa198">&#39; &#39;</span>)
    <span style="color:#b58900">print</span>()</code></pre></div><div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">Theta: 0.517  Phi_0: 0.509  Phi_1: 0.681
Document: red red blue blue cats red cats red dogs cats
Document: dogs red blue blue red dogs dogs cats cats red
Document: cats blue cats red cats red cats blue red red
Document: red red cats cats red cats dogs blue blue dogs
Document: cats red blue dogs cats red dogs red cats cats
Document: red dogs cats red cats cats cats red cats blue
Document: blue dogs blue cats red red dogs dogs red blue
Document: red dogs blue cats blue red red cats cats red
Document: red cats cats red dogs blue cats cats cats cats
Document: cats dogs dogs dogs cats cats cats blue red red
Document: red blue dogs red red cats red cats cats cats
Document: dogs dogs blue dogs cats cats cats cats blue dogs
Document: red dogs cats blue red blue blue blue blue cats
Document: cats blue blue cats cats blue cats cats cats cats
Document: red blue blue dogs cats blue cats blue dogs cats</code></pre></div>
<h3 id="the-dirichlet-distribution">The Dirichlet Distribution</h3>

<p>Before we go on, we need to generalize our model a bit to be able to handle arbitrary numbers of words and topics, instead of being limited to just two. The multivariate generalization of the Bernoulli distribution is the <a href="https://en.wikipedia.org/wiki/Categorical%5Fdistribution">categorical distribution</a>, which simply gives a probability to each of some number of categories. The generalization of the beta distribution is a little trickier. It is called the <a href="https://en.wikipedia.org/wiki/Dirichlet%5Fdistribution">Dirichlet distribution</a>. And just like samples from the beta distribution will give parameters for a Bernoulli RV, samples from the Dirichlet distribution will give parameters for the categorical RV.</p>

<p>Let&rsquo;s recall the two requirements for some set of \(p\)&rsquo;s to be probability parameters to a categorical distribution. First, they have to sum to 1: \(p_0 + p_1 + \cdots + p_v = 1\). This means they form a <a href="https://en.wikipedia.org/wiki/Hyperplane">hyperplane</a> in \(v\)-dimensional space. Second, they all have to be non-negative: \(p_i \geq 0\). This means they all lie in the first quadrant (or <a href="https://en.wikipedia.org/wiki/Orthant">orthant</a>, more precisely). The geometric object that satisfies these two requirements is a <a href="https://en.wikipedia.org/wiki/Simplex#The%5Fstandard%5Fsimplex">simplex</a>. In the case of two variables it will be a line-segment and in the case of three variables it will be a triangle.</p>

<p>As sampled from the distribution, these values will form <a href="https://en.wikipedia.org/wiki/Barycentric%5Fcoordinate%5Fsystem">barycentric coordinates</a> on the simplex. This just means that the coordinates tell you how far the point is from the center of the simplex, instead of how far it is from the origin, like with Cartesian coordinates.</p>

<p>The 3-dimensional Dirichlet returns barycentric coordinates on the 2-simplex, a triangle. We can visualize the surface of the Dirichlet pdf as existing over a triangle; that is, its domain is the simplex.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#719e07">import</span> simplex_plots <span style="color:#719e07">as</span> sp
<span style="color:#586e75"># from https://gist.github.com/tboggs/8778945</span>

alphas <span style="color:#719e07">=</span> [[<span style="color:#2aa198">0.999</span>, <span style="color:#2aa198">0.999</span>, <span style="color:#2aa198">0.999</span>], [<span style="color:#2aa198">1</span>, <span style="color:#2aa198">2</span>, <span style="color:#2aa198">1</span>], [<span style="color:#2aa198">1</span>, <span style="color:#2aa198">2</span>, <span style="color:#2aa198">3</span>],
          [<span style="color:#2aa198">2</span>, <span style="color:#2aa198">0.999</span>, <span style="color:#2aa198">1</span>], [<span style="color:#2aa198">10</span>, <span style="color:#2aa198">3</span>, <span style="color:#2aa198">4</span>], [<span style="color:#2aa198">0.999</span>, <span style="color:#2aa198">1</span>, <span style="color:#2aa198">1</span>]]

fig <span style="color:#719e07">=</span> plt<span style="color:#719e07">.</span>figure(figsize<span style="color:#719e07">=</span>(<span style="color:#2aa198">12</span>, <span style="color:#2aa198">8</span>))
fig<span style="color:#719e07">.</span>suptitle(<span style="color:#2aa198">&#39;The Dirichlet Distribution&#39;</span>, fontsize<span style="color:#719e07">=</span><span style="color:#2aa198">16</span>)
<span style="color:#719e07">for</span> i, a <span style="color:#719e07">in</span> <span style="color:#b58900">enumerate</span>(alphas):
    plt<span style="color:#719e07">.</span>subplot(<span style="color:#2aa198">2</span>, <span style="color:#b58900">len</span>(alphas)<span style="color:#719e07">/</span><span style="color:#2aa198">2</span>, i <span style="color:#719e07">+</span> <span style="color:#2aa198">1</span>)
    sp<span style="color:#719e07">.</span>draw_pdf_contours(sp<span style="color:#719e07">.</span>Dirichlet(a), border<span style="color:#719e07">=</span><span style="color:#cb4b16">True</span>, cmap<span style="color:#719e07">=</span><span style="color:#2aa198">&#39;Blues&#39;</span>)
    title <span style="color:#719e07">=</span> <span style="color:#2aa198">r</span><span style="color:#2aa198">&#39;$\alpha = $ = (</span><span style="color:#2aa198">{0[0]:.3f}</span><span style="color:#2aa198">, </span><span style="color:#2aa198">{0[1]:.3f}</span><span style="color:#2aa198">, </span><span style="color:#2aa198">{0[2]:.3f}</span><span style="color:#2aa198">)&#39;</span><span style="color:#719e07">.</span><span style="color:#b58900">format</span>(a)
    plt<span style="color:#719e07">.</span>title(title, fontdict<span style="color:#719e07">=</span>{<span style="color:#2aa198">&#39;fontsize&#39;</span>: <span style="color:#2aa198">14</span>})</code></pre></div>



<figure>
    
        <img src="//mathformachines.com/post/bayesian-topic-modeling/obipy-resources/14366UEE.png"/> </figure>


<p>Each corner of the triangle will favor a particular category (a word or a topic), just like either side of the domain of the beta distribution favored a category.</p>

<p>As in the upper left picture, whenever all of the entries in \(\alpha\) are equal, we call the distribution &ldquo;symmetric,&rdquo; and whenever they are all less then 1, we call the distribution &ldquo;sparse.&rdquo; Distributions that are both symmetric and sparse are often used as priors when inferring a topic model, symmetry because we don&rsquo;t <em>a priori</em> have any reason to favor one unknown category over another, and sparsity to encourage our categories to be distinct.</p>

<p>Now let&rsquo;s start developing our models.</p>

<h2 id="data-preparation">Data Preparation</h2>

<p>First we&rsquo;ll make up a corpus and put it into an encoding that our models can use. To simplify things, we&rsquo;ll let all of our documents have the same number of tokens and flatten the encoded data structure.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#719e07">from</span> sklearn.preprocessing <span style="color:#719e07">import</span> LabelEncoder
<span style="color:#719e07">from</span> sklearn.feature_extraction.text <span style="color:#719e07">import</span> CountVectorizer

corpus <span style="color:#719e07">=</span> [
    <span style="color:#2aa198">&#39;Red blue green. Green blue blue? Red, red, blue, yellow.&#39;</span>,
    <span style="color:#2aa198">&#39;Car light red stop. Stop car. Car drive green, yellow.&#39;</span>,
    <span style="color:#2aa198">&#39;Car engine gas stop! Battery engine drive, car. Electric, gas.&#39;</span>,
    <span style="color:#2aa198">&#39;Watt, volt, volt, amp. Battery, watt, volt, electric volt charge. &#39;</span>,
]

tokenizer <span style="color:#719e07">=</span> CountVectorizer(lowercase<span style="color:#719e07">=</span><span style="color:#cb4b16">True</span>)<span style="color:#719e07">.</span>build_analyzer()
encoder <span style="color:#719e07">=</span> LabelEncoder()

corpus_tokenized <span style="color:#719e07">=</span> np<span style="color:#719e07">.</span>array([tokenizer(doc) <span style="color:#719e07">for</span> doc <span style="color:#719e07">in</span> corpus])  <span style="color:#586e75"># assign a number to each word</span>
encoder<span style="color:#719e07">.</span>fit(corpus_tokenized<span style="color:#719e07">.</span>ravel())
vocab <span style="color:#719e07">=</span> <span style="color:#b58900">list</span>(encoder<span style="color:#719e07">.</span>classes_)  <span style="color:#586e75"># the vocabulary</span>

<span style="color:#586e75"># The number of documents and their length</span>
D, W <span style="color:#719e07">=</span> corpus_tokenized<span style="color:#719e07">.</span>shape
<span style="color:#586e75"># The number of words in the vocabulary</span>
V <span style="color:#719e07">=</span> <span style="color:#b58900">len</span>(vocab)

<span style="color:#586e75"># Flatten and encode the corpus, and create an index.</span>
data <span style="color:#719e07">=</span> corpus_tokenized<span style="color:#719e07">.</span>ravel()
data <span style="color:#719e07">=</span> encoder<span style="color:#719e07">.</span>transform(data)
data_index <span style="color:#719e07">=</span> np<span style="color:#719e07">.</span>repeat(np<span style="color:#719e07">.</span>arange(D), W)</code></pre></div>
<p>Now a couple of diagnostic functions.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#719e07">def</span> <span style="color:#268bd2">print_top_words</span>(vocab, phis, n):
    <span style="color:#2aa198">&#39;&#39;&#39;Prints the top words occuring within a topic.&#39;&#39;&#39;</span>
    <span style="color:#719e07">for</span> i, p <span style="color:#719e07">in</span> <span style="color:#b58900">enumerate</span>(phis):
        z <span style="color:#719e07">=</span> <span style="color:#b58900">list</span>(<span style="color:#b58900">zip</span>(vocab, p))
        z<span style="color:#719e07">.</span>sort(key <span style="color:#719e07">=</span> <span style="color:#719e07">lambda</span> x: x[<span style="color:#2aa198">1</span>], reverse<span style="color:#719e07">=</span><span style="color:#cb4b16">True</span>)
        z <span style="color:#719e07">=</span> z[<span style="color:#2aa198">0</span>:n]

        <span style="color:#719e07">for</span> word, percent <span style="color:#719e07">in</span> z:
            <span style="color:#b58900">print</span>(f<span style="color:#2aa198">&#39;Topic: </span><span style="color:#2aa198">{i:2}</span><span style="color:#2aa198">  Word: </span><span style="color:#2aa198">{word:10}</span><span style="color:#2aa198">  Percent: </span><span style="color:#2aa198">{percent:0.3f}</span><span style="color:#2aa198">&#39;</span>)

        <span style="color:#b58900">print</span>()

<span style="color:#719e07">def</span> <span style="color:#268bd2">print_corpus_topics</span>(corpus_tokenized, zs):
    <span style="color:#2aa198">&#39;&#39;&#39;Prints the corpus together with the topic assigned to each word.&#39;&#39;&#39;</span>
    <span style="color:#719e07">for</span> d <span style="color:#719e07">in</span> <span style="color:#b58900">range</span>(zs<span style="color:#719e07">.</span>shape[<span style="color:#2aa198">0</span>]):  <span style="color:#586e75"># the document index</span>
        <span style="color:#719e07">for</span> w <span style="color:#719e07">in</span> <span style="color:#b58900">range</span>(zs<span style="color:#719e07">.</span>shape[<span style="color:#2aa198">1</span>]):  <span style="color:#586e75"># the word index</span>
            <span style="color:#b58900">print</span>(f<span style="color:#2aa198">&#39;(</span><span style="color:#2aa198">{corpus_tokenized[d, w]}</span><span style="color:#2aa198">, </span><span style="color:#2aa198">{zs[d, w]}</span><span style="color:#2aa198">)&#39;</span>, end<span style="color:#719e07">=</span><span style="color:#2aa198">&#39; &#39;</span>)
        <span style="color:#b58900">print</span>(<span style="color:#2aa198">&#39;</span><span style="color:#cb4b16">\n</span><span style="color:#2aa198">&#39;</span>)</code></pre></div>
<h3 id="the-unigram-model">The Unigram Model</h3>

<p>In this model, words from every document are drawn from a single categorical distribution.</p>

<p>Distribution of words in a document: \(\phi \sim Dir(\vec{\beta})\), where \(\vec{\beta}\) is a vector of shape parameters</p>

<p>Distribution of tokens: \(W \sim Cat(\vec{\phi})\)</p>

<p><a href="https://en.wikipedia.org/wiki/Markov%5Fchain%5FMonte%5FCarlo">Markov-Chain Monte Carlo</a> is a technique for sampling a model to discover its posterior parameters statistically. When models become complex, it is often the case that analytic solutions for the parameters are intractable. We will use the <a href="https://docs.pymc.io/">PyMC3</a> package.</p>

<p>First we describe the model.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#586e75"># Pseudo-counts for each vocab word occuring in the documents.</span>
beta <span style="color:#719e07">=</span> np<span style="color:#719e07">.</span>ones(V)

<span style="color:#719e07">with</span> pm<span style="color:#719e07">.</span>Model() <span style="color:#719e07">as</span> unigram_model:

    <span style="color:#586e75"># Distribution of word-types in the corpus.</span>
    phi <span style="color:#719e07">=</span> pm<span style="color:#719e07">.</span>Dirichlet(<span style="color:#2aa198">&#39;phi&#39;</span>, a <span style="color:#719e07">=</span> beta)

    <span style="color:#586e75"># The distribution of words.</span>
    w <span style="color:#719e07">=</span> pm<span style="color:#719e07">.</span>Categorical(<span style="color:#2aa198">&#39;w&#39;</span>, p <span style="color:#719e07">=</span> phi, observed <span style="color:#719e07">=</span> data)</code></pre></div>
<p>Next we sample the model to create the posterior distribution.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#719e07">with</span> unigram_model:
    draw <span style="color:#719e07">=</span> <span style="color:#2aa198">5000</span>
    unigram_trace <span style="color:#719e07">=</span> pm<span style="color:#719e07">.</span>sample(<span style="color:#2aa198">5000</span>, tune<span style="color:#719e07">=</span><span style="color:#2aa198">1000</span>, chains<span style="color:#719e07">=</span><span style="color:#2aa198">4</span>, progressbar<span style="color:#719e07">=</span><span style="color:#cb4b16">False</span>)</code></pre></div>
<p>And now we can see what the model determined the proportion of each word in the corpus was.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3">print_top_words(vocab, [unigram_trace<span style="color:#719e07">.</span>get_values(<span style="color:#2aa198">&#39;phi&#39;</span>)[draw<span style="color:#719e07">-</span><span style="color:#2aa198">1</span>]], <span style="color:#b58900">len</span>(vocab))</code></pre></div><div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">Topic:  0  Word: drive       Percent: 0.137
Topic:  0  Word: red         Percent: 0.132
Topic:  0  Word: car         Percent: 0.101
Topic:  0  Word: volt        Percent: 0.094
Topic:  0  Word: stop        Percent: 0.078
Topic:  0  Word: light       Percent: 0.066
Topic:  0  Word: engine      Percent: 0.060
Topic:  0  Word: blue        Percent: 0.060
Topic:  0  Word: green       Percent: 0.053
Topic:  0  Word: charge      Percent: 0.046
Topic:  0  Word: amp         Percent: 0.045
Topic:  0  Word: yellow      Percent: 0.032
Topic:  0  Word: electric    Percent: 0.029
Topic:  0  Word: gas         Percent: 0.028
Topic:  0  Word: watt        Percent: 0.027
Topic:  0  Word: battery     Percent: 0.013</code></pre></div>
<h3 id="mixture-of-unigrams--naive-bayes">Mixture of Unigrams (Naive Bayes)</h3>

<p>In this model, each document is assigned a topic and each topic has its own distribution of words.</p>

<p>Distribution of topics to documents: \(\vec{\theta} \sim Dirichlet(\vec{\alpha})\)</p>

<p>Distribution of words to topics: \(\vec{\phi} \sim Dirichlet(\vec{\beta})\)</p>

<p>The topics: \(T \sim Categorical(\vec{\theta})\)</p>

<p>The tokens: \(W \sim Categorical(\vec{\phi})\)</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#586e75"># Number of topics</span>
K <span style="color:#719e07">=</span> <span style="color:#2aa198">3</span>

<span style="color:#586e75"># Pseudo-counts for topics and words.</span>
alpha <span style="color:#719e07">=</span> np<span style="color:#719e07">.</span>ones(K)<span style="color:#719e07">*</span><span style="color:#2aa198">0.8</span>
beta <span style="color:#719e07">=</span> np<span style="color:#719e07">.</span>ones(V)<span style="color:#719e07">*</span><span style="color:#2aa198">0.8</span>

<span style="color:#719e07">with</span> pm<span style="color:#719e07">.</span>Model() <span style="color:#719e07">as</span> naive_model:
    <span style="color:#586e75"># Global topic distribution</span>
    theta <span style="color:#719e07">=</span> pm<span style="color:#719e07">.</span>Dirichlet(<span style="color:#2aa198">&#34;theta&#34;</span>, a<span style="color:#719e07">=</span>alpha)

    <span style="color:#586e75"># Word distributions for K topics</span>
    phi <span style="color:#719e07">=</span> pm<span style="color:#719e07">.</span>Dirichlet(<span style="color:#2aa198">&#34;phi&#34;</span>, a<span style="color:#719e07">=</span>beta, shape<span style="color:#719e07">=</span>(K, V))

    <span style="color:#586e75"># Topic of documents</span>
    z <span style="color:#719e07">=</span> pm<span style="color:#719e07">.</span>Categorical(<span style="color:#2aa198">&#34;z&#34;</span>, p<span style="color:#719e07">=</span>theta, shape<span style="color:#719e07">=</span>D)

    <span style="color:#586e75"># Words in documents</span>
    p <span style="color:#719e07">=</span> phi[z][data_index]
    w <span style="color:#719e07">=</span> pm<span style="color:#719e07">.</span>Categorical(<span style="color:#2aa198">&#34;w&#34;</span>, p<span style="color:#719e07">=</span>p, observed<span style="color:#719e07">=</span>data)</code></pre></div><div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#719e07">with</span> naive_model:
    draw <span style="color:#719e07">=</span> <span style="color:#2aa198">5000</span>
    naive_trace <span style="color:#719e07">=</span> pm<span style="color:#719e07">.</span>sample(draw, tune<span style="color:#719e07">=</span><span style="color:#2aa198">1000</span>, chains<span style="color:#719e07">=</span><span style="color:#2aa198">4</span>, progressbar<span style="color:#719e07">=</span><span style="color:#cb4b16">False</span>)</code></pre></div><div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3">print_top_words(vocab, naive_trace[<span style="color:#2aa198">&#39;phi&#39;</span>][draw<span style="color:#719e07">-</span><span style="color:#2aa198">1</span>], <span style="color:#2aa198">5</span>)</code></pre></div><div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">Topic:  0  Word: car         Percent: 0.198
Topic:  0  Word: amp         Percent: 0.193
Topic:  0  Word: gas         Percent: 0.150
Topic:  0  Word: electric    Percent: 0.141
Topic:  0  Word: volt        Percent: 0.087

Topic:  1  Word: volt        Percent: 0.196
Topic:  1  Word: drive       Percent: 0.186
Topic:  1  Word: battery     Percent: 0.135
Topic:  1  Word: red         Percent: 0.108
Topic:  1  Word: blue        Percent: 0.063

Topic:  2  Word: green       Percent: 0.250
Topic:  2  Word: stop        Percent: 0.139
Topic:  2  Word: engine      Percent: 0.108
Topic:  2  Word: drive       Percent: 0.100
Topic:  2  Word: red         Percent: 0.078</code></pre></div>
<p>We can also see the topic the model assigned to each document.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#719e07">for</span> d, t <span style="color:#719e07">in</span> <span style="color:#b58900">enumerate</span>(naive_trace[<span style="color:#2aa198">&#39;z&#39;</span>][draw<span style="color:#719e07">-</span><span style="color:#2aa198">1</span>]):
    <span style="color:#b58900">print</span>(<span style="color:#2aa198">&#39;Document: </span><span style="color:#2aa198">{}</span><span style="color:#2aa198">  Topic: </span><span style="color:#2aa198">{}</span><span style="color:#2aa198">&#39;</span><span style="color:#719e07">.</span><span style="color:#b58900">format</span>(d, t))</code></pre></div><div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">Document: 0  Topic: 1
Document: 1  Topic: 2
Document: 2  Topic: 0
Document: 3  Topic: 1</code></pre></div>
<h3 id="latent-dirichlet-allocation-1">Latent Dirichlet Allocation</h3>

<p>In this model, each word is assigned a topic and topics are distributed varyingly within each document.</p>

<p>Distribution of topics within documents: \(\vec{\theta} \sim Dirichlet(\vec{\alpha})\)</p>

<p>Distribution of words to topics: \(\vec{\phi} \sim Dirichlet(\vec{\beta})\)</p>

<p>The topics: \(T \sim Categorical(\vec{\theta})\)</p>

<p>The tokens: \(W \sim Categorical(\vec{\phi})\)</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#586e75"># Number of topics</span>
K <span style="color:#719e07">=</span> <span style="color:#2aa198">3</span>

<span style="color:#586e75"># Pseudo-counts. Sparse to encourage separation.</span>
alpha <span style="color:#719e07">=</span> np<span style="color:#719e07">.</span>ones((<span style="color:#2aa198">1</span>, K))<span style="color:#719e07">*</span><span style="color:#2aa198">0.5</span>
beta <span style="color:#719e07">=</span> np<span style="color:#719e07">.</span>ones((<span style="color:#2aa198">1</span>, V))<span style="color:#719e07">*</span><span style="color:#2aa198">0.5</span>

<span style="color:#719e07">with</span> pm<span style="color:#719e07">.</span>Model() <span style="color:#719e07">as</span> lda_model:
    <span style="color:#586e75"># Distribution of topics within each document</span>
    theta <span style="color:#719e07">=</span> pm<span style="color:#719e07">.</span>Dirichlet(<span style="color:#2aa198">&#34;theta&#34;</span>, a<span style="color:#719e07">=</span>alpha, shape<span style="color:#719e07">=</span>(D, K))

    <span style="color:#586e75"># Distribution of words within each topic</span>
    phi <span style="color:#719e07">=</span> pm<span style="color:#719e07">.</span>Dirichlet(<span style="color:#2aa198">&#34;phi&#34;</span>, a<span style="color:#719e07">=</span>beta, shape<span style="color:#719e07">=</span>(K, V))

    <span style="color:#586e75"># The topic for each word</span>
    z <span style="color:#719e07">=</span> pm<span style="color:#719e07">.</span>Categorical(<span style="color:#2aa198">&#34;z&#34;</span>, p<span style="color:#719e07">=</span>theta, shape<span style="color:#719e07">=</span>(W, D))

    <span style="color:#586e75"># Words in documents</span>
    p <span style="color:#719e07">=</span> phi[z]<span style="color:#719e07">.</span>reshape((D<span style="color:#719e07">*</span>W, V))
    w <span style="color:#719e07">=</span> pm<span style="color:#719e07">.</span>Categorical(<span style="color:#2aa198">&#34;w&#34;</span>, p<span style="color:#719e07">=</span>p, observed<span style="color:#719e07">=</span>data)</code></pre></div><div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#719e07">with</span> lda_model:
    draw <span style="color:#719e07">=</span> <span style="color:#2aa198">5000</span>
    lda_trace <span style="color:#719e07">=</span> pm<span style="color:#719e07">.</span>sample(draw, tune<span style="color:#719e07">=</span><span style="color:#2aa198">1000</span>, chains<span style="color:#719e07">=</span><span style="color:#2aa198">4</span>, progressbar<span style="color:#719e07">=</span><span style="color:#cb4b16">False</span>)

print_top_words(tokens, lda_trace<span style="color:#719e07">.</span>get_values(<span style="color:#2aa198">&#39;phi&#39;</span>)[draw<span style="color:#719e07">-</span><span style="color:#2aa198">1</span>], <span style="color:#2aa198">4</span>)</code></pre></div><div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">Topic:  0  Word:          0  Percent: 0.134
Topic:  0  Word:          1  Percent: 0.129
Topic:  0  Word:          1  Percent: 0.052
Topic:  0  Word:          1  Percent: 0.014

Topic:  1  Word:          1  Percent: 0.156
Topic:  1  Word:          1  Percent: 0.126
Topic:  1  Word:          0  Percent: 0.112
Topic:  1  Word:          1  Percent: 0.081

Topic:  2  Word:          0  Percent: 0.291
Topic:  2  Word:          0  Percent: 0.163
Topic:  2  Word:          0  Percent: 0.135
Topic:  2  Word:          0  Percent: 0.087</code></pre></div>
<p>At the cost of some complexity, we can rewrite our model to handle a corpus with documents of varying lengths.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3">alpha <span style="color:#719e07">=</span> np<span style="color:#719e07">.</span>ones([D, K])<span style="color:#719e07">*</span><span style="color:#2aa198">0.5</span>  <span style="color:#586e75"># prior weights for the topics in each document (pseudo-counts)</span>
beta  <span style="color:#719e07">=</span> np<span style="color:#719e07">.</span>ones([K, V])<span style="color:#719e07">*</span><span style="color:#2aa198">0.5</span>  <span style="color:#586e75"># prior weights for the vocab words in each topic (pseudo-counts)</span>

sequence_data <span style="color:#719e07">=</span> np<span style="color:#719e07">.</span>reshape(np<span style="color:#719e07">.</span>array(data), (D,W))
N <span style="color:#719e07">=</span> np<span style="color:#719e07">.</span>repeat(W, D)  <span style="color:#586e75"># this model needs a list of document lengths</span>

<span style="color:#719e07">with</span> pm<span style="color:#719e07">.</span>Model() <span style="color:#719e07">as</span> sequence_model:

    <span style="color:#586e75"># distribution of the topics occuring in a particular document</span>
    theta   <span style="color:#719e07">=</span> pm<span style="color:#719e07">.</span>Dirichlet(<span style="color:#2aa198">&#39;theta&#39;</span>, a<span style="color:#719e07">=</span>alpha, shape<span style="color:#719e07">=</span>(D, K))

    <span style="color:#586e75"># distribution of the vocab words occuring in a particular topic</span>
    phi     <span style="color:#719e07">=</span> pm<span style="color:#719e07">.</span>Dirichlet(<span style="color:#2aa198">&#39;phi&#39;</span>, a<span style="color:#719e07">=</span>beta, shape<span style="color:#719e07">=</span>(K, V))

    <span style="color:#586e75"># the topic for a particular word in a particular document: shape = (D, N[d])</span>
    <span style="color:#586e75"># theta[d] is the vector of category probabilities for each topic in</span>
    <span style="color:#586e75"># document d.</span>
    z <span style="color:#719e07">=</span> [pm<span style="color:#719e07">.</span>Categorical(<span style="color:#2aa198">&#39;z_</span><span style="color:#2aa198">{}</span><span style="color:#2aa198">&#39;</span><span style="color:#719e07">.</span><span style="color:#b58900">format</span>(d), p <span style="color:#719e07">=</span> theta[d], shape<span style="color:#719e07">=</span>N[d])
          <span style="color:#719e07">for</span> d <span style="color:#719e07">in</span> <span style="color:#b58900">range</span>(D)]

    <span style="color:#586e75"># the word occuring at position n, in a particular document d: shape = (D, N[d])</span>
    <span style="color:#586e75"># z[d] is the vector of topics for document d</span>
    <span style="color:#586e75"># z[d][n] is the topic for word n in document d</span>
    <span style="color:#586e75"># phi[z[d][n]] is the distribution of words for topic z[d][n]</span>
    <span style="color:#586e75"># [d][n] is the n-th word observed in document d</span>
    w <span style="color:#719e07">=</span> [pm<span style="color:#719e07">.</span>Categorical(<span style="color:#2aa198">&#39;w_</span><span style="color:#2aa198">{}</span><span style="color:#2aa198">_</span><span style="color:#2aa198">{}</span><span style="color:#2aa198">&#39;</span><span style="color:#719e07">.</span><span style="color:#b58900">format</span>(d, n), p<span style="color:#719e07">=</span>phi[z[d][n]],
                        observed <span style="color:#719e07">=</span> sequence_data[d][n])
         <span style="color:#719e07">for</span> d <span style="color:#719e07">in</span> <span style="color:#b58900">range</span>(D) <span style="color:#719e07">for</span> n <span style="color:#719e07">in</span> <span style="color:#b58900">range</span>(N[d])]

<span style="color:#719e07">with</span> sequence_model:
    draw <span style="color:#719e07">=</span> <span style="color:#2aa198">5000</span>
    sequence_trace <span style="color:#719e07">=</span> pm<span style="color:#719e07">.</span>sample(draw, tune<span style="color:#719e07">=</span><span style="color:#2aa198">1000</span>, chains<span style="color:#719e07">=</span><span style="color:#2aa198">4</span>, progressbar<span style="color:#719e07">=</span><span style="color:#cb4b16">False</span>)

print_top_words(tokens, sequence_trace<span style="color:#719e07">.</span>get_values(<span style="color:#2aa198">&#39;phi&#39;</span>)[<span style="color:#2aa198">4999</span>], <span style="color:#2aa198">4</span>)</code></pre></div>
<p>And here we can see what topic the model assigned to each token in the corpus.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3">zs <span style="color:#719e07">=</span> [sequence_trace<span style="color:#719e07">.</span>get_values(<span style="color:#2aa198">&#39;z_</span><span style="color:#2aa198">{}</span><span style="color:#2aa198">&#39;</span><span style="color:#719e07">.</span><span style="color:#b58900">format</span>(d))[draw<span style="color:#719e07">-</span><span style="color:#2aa198">1</span>] <span style="color:#719e07">for</span> d <span style="color:#719e07">in</span> <span style="color:#b58900">range</span>(D)]
zs <span style="color:#719e07">=</span> np<span style="color:#719e07">.</span>array(zs)

print_corpus_topics(corpus_tokenized, zs)</code></pre></div><div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">(red, 2) (blue, 2) (green, 2) (green, 2) (blue, 2) (blue, 0) (red, 2) (red, 2) (blue, 2) (yellow, 2)

(car, 0) (light, 1) (red, 0) (stop, 0) (stop, 2) (car, 0) (car, 1) (drive, 0) (green, 0) (yellow, 2)

(car, 0) (engine, 0) (gas, 0) (stop, 0) (battery, 0) (engine, 0) (drive, 0) (car, 0) (electric, 0) (gas, 0)

(watt, 1) (volt, 1) (volt, 1) (amp, 1) (battery, 1) (watt, 1) (volt, 1) (electric, 0) (volt, 1) (charge, 0)</code></pre></div>
<p>Since we chose to distribute words among three topics, we can examine the distributions of these topics to each document on a simplex. Below, each triangle represents a document and each corner represents a topic. Whenever the sampled points cluster at a corner, that means our model decided that that document was predominantly about the corresponding topic.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#719e07">with</span> sequence_model:
    pps <span style="color:#719e07">=</span> pm<span style="color:#719e07">.</span>sample_posterior_predictive(sequence_trace, <span style="color:#b58900">vars</span><span style="color:#719e07">=</span>[theta], samples<span style="color:#719e07">=</span><span style="color:#2aa198">1000</span>, progressbar<span style="color:#719e07">=</span><span style="color:#cb4b16">False</span>)

var <span style="color:#719e07">=</span> pps[<span style="color:#2aa198">&#39;theta&#39;</span>]
thetas <span style="color:#719e07">=</span> sequence_trace[<span style="color:#2aa198">&#39;theta&#39;</span>][<span style="color:#2aa198">4999</span>]
nthetas <span style="color:#719e07">=</span> thetas<span style="color:#719e07">.</span>shape[<span style="color:#2aa198">0</span>]

blue <span style="color:#719e07">=</span> sns<span style="color:#719e07">.</span>color_palette(<span style="color:#2aa198">&#39;Blues_r&#39;</span>)[<span style="color:#2aa198">0</span>]
fig <span style="color:#719e07">=</span> plt<span style="color:#719e07">.</span>figure(figsize<span style="color:#719e07">=</span>(<span style="color:#2aa198">8</span>, <span style="color:#2aa198">8</span>))
fig<span style="color:#719e07">.</span>suptitle(<span style="color:#2aa198">&#39;Distribution of Topics to Documents&#39;</span>, fontsize<span style="color:#719e07">=</span><span style="color:#2aa198">16</span>)
<span style="color:#719e07">for</span> i, ts <span style="color:#719e07">in</span> <span style="color:#b58900">enumerate</span>(thetas):
    plt<span style="color:#719e07">.</span>subplot(<span style="color:#2aa198">2</span>, nthetas<span style="color:#719e07">/</span><span style="color:#2aa198">2</span>, i <span style="color:#719e07">+</span> <span style="color:#2aa198">1</span>)
    sp<span style="color:#719e07">.</span>plot_points(var[:,i], color<span style="color:#719e07">=</span>blue, marker<span style="color:#719e07">=</span><span style="color:#2aa198">&#39;o&#39;</span>, alpha<span style="color:#719e07">=</span><span style="color:#2aa198">0.1</span>, markersize<span style="color:#719e07">=</span><span style="color:#2aa198">3</span>)
    title <span style="color:#719e07">=</span> <span style="color:#2aa198">r</span><span style="color:#2aa198">&#39;$\theta_</span><span style="color:#2aa198">{0}</span><span style="color:#2aa198">$ = (</span><span style="color:#2aa198">{1[0]:.3f}</span><span style="color:#2aa198">, </span><span style="color:#2aa198">{1[1]:.3f}</span><span style="color:#2aa198">, </span><span style="color:#2aa198">{1[2]:.3f}</span><span style="color:#2aa198">)&#39;</span><span style="color:#719e07">.</span><span style="color:#b58900">format</span>(i,ts)
    plt<span style="color:#719e07">.</span>title(title, fontdict<span style="color:#719e07">=</span>{<span style="color:#2aa198">&#39;fontsize&#39;</span>: <span style="color:#2aa198">14</span>})</code></pre></div>



<figure>
    
        <img src="//mathformachines.com/post/bayesian-topic-modeling/obipy-resources/14366hOK.png"/> </figure>


<p>That&rsquo;s all for now!</p>

<h2 id="references">References</h2>

<p>Blei, David M, Andrew Y Ng and Michael I Jordan. 2003.  Latent dirichlet allocation. Journal of machine Learning research.</p>

<p><a href="https://stackoverflow.com/questions/31473459/pymc3-how-to-implement-latent-dirichlet-allocation">https://stackoverflow.com/questions/31473459/pymc3-how-to-implement-latent-dirichlet-allocation</a></p>

<p><a href="https://github.com/junpenglao/Planet%5FSakaar%5FData%5FScience/blob/master/PyMC3QnA/discourse%5F2314.ipynb">https://github.com/junpenglao/Planet%5FSakaar%5FData%5FScience/blob/master/PyMC3QnA/discourse%5F2314.ipynb</a></p>

  </section>

  <section>
    
      
    
  </section>
</article>

      </div>

      <div id="side" class="pr-1 bg-white">
        <aside class="pr-3">
          
  
    <div id="toc" class="Box Box--blue mb-3">
      <b>Bayesian Topic Modeling</b>
      <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#some-simple-generative-examples">Some Simple Generative Examples</a>
<ul>
<li><a href="#unigram-model">Unigram Model</a></li>
<li><a href="#mixture-of-unigrams">Mixture of Unigrams</a></li>
<li><a href="#latent-dirichlet-allocation">Latent Dirichlet Allocation</a></li>
<li><a href="#the-dirichlet-distribution">The Dirichlet Distribution</a></li>
</ul></li>
<li><a href="#data-preparation">Data Preparation</a>
<ul>
<li><a href="#the-unigram-model">The Unigram Model</a></li>
<li><a href="#mixture-of-unigrams--naive-bayes">Mixture of Unigrams (Naive Bayes)</a></li>
<li><a href="#latent-dirichlet-allocation-1">Latent Dirichlet Allocation</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul></li>
</ul>
</nav>
    </div>
  

  
    <div>
      
    </div>
  

        </aside>
      </div>

      <div id="footer" class="pt-2 pb-3 bg-white text-center">
        

  <span class="text-small text-gray">
    &copy;Ryan Holbrook 2019 &middot; 

    Powered by the
    <a href="https://github.com/qqhann/hugo-primer" class="link-gray-dark">Hugo-Primer</a> theme for
    <a href="https://gohugo.io" class="link-gray-dark">Hugo</a>.
  </span>

    
    
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
    </script>


    <script type="text/x-mathjax-config"> 
      MathJax.Hub.Config({ 
         "HTML-CSS": { scale: 90, linebreaks: { automatic: true } }, 
          SVG: { linebreaks: { automatic:true } }, 
      });
    </script>


    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-133546767-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-133546767-1');
    </script>
  </body>
</html>

      </div>
    </div>


    
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });</script>
  </body>
</html>

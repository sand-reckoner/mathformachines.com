<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tutorial on Math for Machines</title>
    <link>https://mathformachines.com/categories/tutorial/</link>
    <description>Recent content in Tutorial on Math for Machines</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy;Ryan Holbrook 2019</copyright>
    <lastBuildDate>Mon, 18 Mar 2019 07:28:00 -0500</lastBuildDate>
    
	<atom:link href="https://mathformachines.com/categories/tutorial/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Change of Basis for Vectors and Covectors</title>
      <link>https://mathformachines.com/posts/change-of-basis/</link>
      <pubDate>Mon, 18 Mar 2019 07:28:00 -0500</pubDate>
      
      <guid>https://mathformachines.com/posts/change-of-basis/</guid>
      <description>We share a philosophy about linear algebra: we think basis-free, we write basis-free, but when the chips are down we close the office door and compute with matrices like fury.
 - Irving Kaplansky  mathoverflow.net/questions/...    Often, the first step in analyzing a problem is to transform it into something more amenable to our analysis. We would like the representation of our problem to reflect as naturally as possible whatever features of it we are most interested in.</description>
    </item>
    
    <item>
      <title>A Tour of Tensors</title>
      <link>https://mathformachines.com/posts/a-tour-of-tensors/</link>
      <pubDate>Tue, 05 Feb 2019 12:59:00 -0600</pubDate>
      
      <guid>https://mathformachines.com/posts/a-tour-of-tensors/</guid>
      <description>Here are some notes on tensors, starting from elementary linear algebra. I&amp;rsquo;ve tried to take a computational focus and to avoid formalism when possible. If you&amp;rsquo;re interested in tensors applied to machine learning, or have wondered why arrays in Tensorflow are called tensors, you might find this useful. I&amp;rsquo;ll do some computations in Sage and also in Numpy for illustration.
Abstract Tensors First, let&amp;rsquo;s take brief look at tensors in the abstract.</description>
    </item>
    
    <item>
      <title>Bayesian Topic Modeling</title>
      <link>https://mathformachines.com/posts/bayesian-topic-modeling/</link>
      <pubDate>Wed, 30 Jan 2019 05:57:00 -0600</pubDate>
      
      <guid>https://mathformachines.com/posts/bayesian-topic-modeling/</guid>
      <description>Imagine we have some collection of documents. They could be novels, or tweets, or financial reports&amp;mdash;just any collection of text. We want an algorithm that can discover what they are about, and we would like our algorithm to do it automatically, without any hints. (That is, we want our algorithm to be unsupervised.) We will look at several models that probabilistically assign words to topics using Bayes&amp;rsquo; Theorem. They are all Bayesian Graphical Models.</description>
    </item>
    
  </channel>
</rss>
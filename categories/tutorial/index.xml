<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tutorial on Math for Machines</title>
    <link>https://mathformachines.com/categories/tutorial/</link>
    <description>Recent content in Tutorial on Math for Machines</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy;Ryan Holbrook 2019</copyright>
    <lastBuildDate>Mon, 18 Mar 2019 07:28:00 -0500</lastBuildDate>
    
	<atom:link href="https://mathformachines.com/categories/tutorial/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Change of Basis for Vectors and Covectors</title>
      <link>https://mathformachines.com/posts/change-of-basis/</link>
      <pubDate>Mon, 18 Mar 2019 07:28:00 -0500</pubDate>
      
      <guid>https://mathformachines.com/posts/change-of-basis/</guid>
      <description>&lt;blockquote&gt;
  &lt;p&gt;We share a philosophy about linear algebra: we think basis-free, we write basis-free, but when the chips are down we close the office door and compute with matrices like fury.&lt;/p&gt;
  &lt;footer&gt;
    &lt;strong&gt;- Irving Kaplansky&lt;/strong&gt;
    
      
        &lt;cite&gt;
          &lt;a href=&#34;#ZgotmplZ&#34; title=&#34;&amp;lt;https://mathoverflow.net/questions/11669/what-is-the-difference-between-matrix-theory-and-linear-algebra/19923&amp;gt;&#34;&gt;mathoverflow.net/questions/...&lt;/a&gt; 
        &lt;/cite&gt;
      
    
  &lt;/footer&gt;
&lt;/blockquote&gt;


&lt;p&gt;Often, the first step in analyzing a problem is to &lt;em&gt;transform&lt;/em&gt; it into something more amenable to our analysis. We would like the &lt;em&gt;representation&lt;/em&gt; of our problem to reflect as naturally as possible whatever features of it we are most interested in. We might normalize data through a scaling transform, for instance, to eliminate spurious differences among like quantities. Or we might rotate data to align some of its salient dimensions with the coordinate axes, simplifying computations. Many matrix decompositions take the form \(M = BNA\). When \(B\) and \(A\) are non-singular, we can think of \(N\) as being a simpler representation of \(M\) under coordinate transforms \(B\) and \(A\). The &lt;a href=&#34;https://en.wikipedia.org/wiki/Eigendecomposition%5Fof%5Fa%5Fmatrix&#34;&gt;spectral decomposition&lt;/a&gt; and the &lt;a href=&#34;https://en.wikipedia.org/wiki/Singular%5Fvalue%5Fdecomposition&#34;&gt;singular value decomposition&lt;/a&gt; are of this form.&lt;/p&gt;

&lt;p&gt;All of these kinds of coordinate transformations are &lt;em&gt;linear&lt;/em&gt; transformations. Linear coordinate transformations come about from operations on basis vectors that leave any vectors represented by them &lt;a href=&#34;https://en.wikipedia.org/wiki/Active%5Fand%5Fpassive%5Ftransformation&#34;&gt;unchanged&lt;/a&gt;. They are, in other words, a change of basis.&lt;/p&gt;

&lt;p&gt;This post came about from my frustration at not finding simple formulas for these transformations with simple explanations to go along with them. So here, I tried to give a simple exposition of coordinate transformations for vectors in vector spaces along with transformations of their cousins, the covectors in the dual space. I&amp;rsquo;ll get into matrices and some applications in a future post.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A Tour of Tensors</title>
      <link>https://mathformachines.com/posts/a-tour-of-tensors/</link>
      <pubDate>Tue, 05 Feb 2019 12:59:00 -0600</pubDate>
      
      <guid>https://mathformachines.com/posts/a-tour-of-tensors/</guid>
      <description>&lt;p&gt;Tensors can sometimes have a fearsome reputation. They are at heart, however, no more difficult to define than polynomials. I&amp;rsquo;ve tried in these notes to take a computational focus and to avoid formalism when possible; I haven&amp;rsquo;t assumed any more than what you might encounter in an undergraduate linear algebra course. If you&amp;rsquo;re interested in tensors applied to machine learning, or have wondered why arrays in Tensorflow are called tensors, you might find this useful. I&amp;rsquo;ll do some computations in &lt;a href=&#34;http://www.sagemath.org/&#34;&gt;Sage&lt;/a&gt; and also in &lt;a href=&#34;http://www.numpy.org/&#34;&gt;Numpy&lt;/a&gt; for illustration.&lt;/p&gt;

&lt;h2 id=&#34;abstract-tensors&#34;&gt;Abstract Tensors&lt;/h2&gt;

&lt;p&gt;First, let&amp;rsquo;s take brief look at tensors in the abstract. This is just to give us an idea of what properties they have and how they function. I&amp;rsquo;ll gloss over most of the details of the construction.&lt;/p&gt;

&lt;p&gt;A tensor is a vector. It is an element of a vector space. Being a vector, if we have a basis for the space we can write the tensor as a list of coordinates (or maybe something like a matrix or an array â€“ we&amp;rsquo;ll see how).&lt;/p&gt;

&lt;p&gt;A tensor is a vector in a product vector space. This means that part of it comes from one vector space and part of it comes from another. These parts combine in a way that fits with the usual notions of how products should work. Why would we want these tensors, these products of vectors? It turns out that lots of useful things are tensors. Matrices and linear maps are tensors, and so are determinants and inner products and cross products. Tensors give us power to express many useful ideas.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Bayesian Topic Modeling</title>
      <link>https://mathformachines.com/posts/bayesian-topic-modeling/</link>
      <pubDate>Wed, 30 Jan 2019 05:57:00 -0600</pubDate>
      
      <guid>https://mathformachines.com/posts/bayesian-topic-modeling/</guid>
      <description>&lt;p&gt;Imagine we have some collection of documents. They could be novels, or tweets, or financial reports&amp;mdash;just any collection of text. We want an algorithm that can discover what they are about, and we would like our algorithm to do it automatically, without any hints. (That is, we want our algorithm to be &lt;em&gt;unsupervised&lt;/em&gt;.) We will look at several models that probabilistically assign words to topics using &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayes&#39;%5Ftheorem&#34;&gt;Bayes&amp;rsquo; Theorem&lt;/a&gt;. They are all &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayesian%5Fnetwork&#34;&gt;Bayesian Graphical Models&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The basic problem in statistics is to infer some unobservable value from observable instances of it. In our case, we want to infer the &lt;em&gt;topics&lt;/em&gt; of a document from the actual words in the document. We want to be able to infer that our document is about &amp;ldquo;colors&amp;rdquo; if we observe &amp;ldquo;red&amp;rdquo; and &amp;ldquo;green&amp;rdquo; and &amp;ldquo;blue&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Bayes&amp;rsquo; Theorem allows us to do this. It allows us to infer probabilities concerning the unobserved value from the observations that we can make. It allows us to reason backwards in some sense. So, when constructing a Bayesian model, it is helpful to &lt;em&gt;think&lt;/em&gt; backwards. Instead of first asking how words are distributed to topics and topics to documents, we will ask how we could &lt;em&gt;generate&lt;/em&gt; a document if we already knew these distributions. To construct our model, we will first reason from the unknown values to the known values so that we know how to do the converse when the time comes.
&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>